{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Image Caption Generator CNN-RNN Model\n",
    "\n",
    "The Image Caption Generator CNN-RNN model using CNN and LSTM\n",
    "- CNN: used for extracting features from image\n",
    "- LSTM: used to genreate description of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139b09d41b4c4f95a3af1ebd1115062e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# small library for seeing the progress of loops.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "**load_doc( filename )** – For loading the document file and reading the contents inside the file into a string.\n",
    "\n",
    "**all_img_captions( filename )** – This function will create a descriptions dictionary that maps images with a list of 5 captions. The descriptions dictionary will look something like this:\n",
    "\n",
    "**cleaning_text( descriptions)** – This function takes all descriptions and performs data cleaning. This is an important step when we work with textual data, according to our goal, we decide what type of cleaning we want to perform on the text. In our case, we will be removing punctuations, converting all text to lowercase and removing words that contain numbers. So, a caption like “A man riding on a three-wheeled wheelchair” will be transformed into “man riding on three wheeled wheelchair”\n",
    "\n",
    "**text_vocabulary( descriptions )** – This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions.\n",
    "\n",
    "**save_descriptions( descriptions, filename )** – This function will create a list of all the descriptions that have been preprocessed and store them into a file. We will create a descriptions.txt file to store all the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loads a text file `filename` into memory\n",
    "'''\n",
    "def load_doc(filename):\n",
    "    # Opening the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    # extract all the text in the file to return\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "'''\n",
    "Get all imgs with their captions and \n",
    "returns a dictionary of all caption from `filename`.\n",
    "'''\n",
    "def all_img_captions(filename):\n",
    "    ## Extract teext from the file\n",
    "    file = load_doc(filename)\n",
    "    ## Split each caption to store as description dictionary\n",
    "    captions = file.split('\\n')\n",
    "    descriptions ={}\n",
    "    \n",
    "    for caption in captions[:-1]:\n",
    "        ## split an image from its caption\n",
    "        img, caption = caption.split('\\t')\n",
    "        ## Map the image to the caption in descripions\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [ caption ]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n",
    "\n",
    "'''\n",
    "Data cleaning- \n",
    "- lower casing\n",
    "- removing puntuations\n",
    "- words containing numbers\n",
    "'''\n",
    "def cleaning_text(captions):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    for img,caps in captions.items():\n",
    "        for i,img_caption in enumerate(caps):\n",
    "\n",
    "            img_caption.replace(\"-\",\" \")\n",
    "            desc = img_caption.split()\n",
    "\n",
    "            #converts to lowercase\n",
    "            desc = [word.lower() for word in desc]\n",
    "            \n",
    "            #remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            \n",
    "            #remove hanging 's and a \n",
    "            desc = [word for word in desc if(len(word)>1)]\n",
    "            \n",
    "            #remove tokens with numbers in them\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "            \n",
    "            #convert back to string and store it back to its original caption\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i]= img_caption\n",
    "    return captions\n",
    "\n",
    "'''\n",
    "Separates all the unique words and \n",
    "create the vocabulary from all the descriptions.\n",
    "`descriptions` result from all_img_captions(filename)\n",
    "'''\n",
    "def text_vocabulary(descriptions):\n",
    "    # build vocabulary of all unique words\n",
    "    vocab = set()\n",
    "\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "'''\n",
    "Creates a list of all the descriptions that \n",
    "have been preprocessed and store them into a file.\n",
    "\n",
    "All descriptions in one file \n",
    "'''\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    \n",
    "    ## Stringify entire descriptions to prepare to write to file\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc )\n",
    "    data = \"\\n\".join(lines)\n",
    "    \n",
    "    ## Write to and close file after writing to it\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFlicker8k_Dataset\u001b[m\u001b[m    \u001b[34mFlickr8k_text\u001b[m\u001b[m\n",
      "Flickr8k_Dataset.zip Flickr8k_text.zip\n",
      "ls: .txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls data\n",
    "\n",
    "!ls data/Flicker8k_Dataset > grep * .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: dat: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of descripions:  8092\n"
     ]
    }
   ],
   "source": [
    "## Specify paths for Flicker data\n",
    "dataset_text = 'data/Flickr8k_text'\n",
    "dataset_images = 'data/Flicker8k_Dataset'\n",
    "\n",
    "## prepare text data\n",
    "text_fn = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "\n",
    "## Load fn that contains all data and map into descriptions dictionary\n",
    "descriptions = all_img_captions(text_fn)\n",
    "print('Length of descripions: ', len(descriptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary =  8763\n"
     ]
    }
   ],
   "source": [
    "## Clean descriptions\n",
    "cleaned_descriptions = cleaning_text(descriptions)\n",
    "\n",
    "## Building unique vocabulary \n",
    "vocabulary = text_vocabulary(cleaned_descriptions)\n",
    "print(\"Length of vocabulary = \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all descriptoins to the 'decriptions.txt' file\n",
    "dest_fn = 'descriptions.txt'\n",
    "save_descriptions(cleaned_descriptions, dest_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN: Extracting feature vector from all images\n",
    "\n",
    "- Will take advatange of transfer learning so we can start from a pre-trained model\n",
    "- Using Xception mode which was trained on `imageenet` with 1000 diffrenet classes to classify\n",
    "\n",
    "`extract_features()` will extract features for all images and map image names with their respective feature array. This will theen dump features dictionary into 'features.p' pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmodel = Xception(include_top=False, \n",
    "                  pooling='avg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    Xmodel = Xception( include_top=False, pooling='avg')\n",
    "    \n",
    "    features = {}\n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        fn = directory + \"/\" + img\n",
    "        \n",
    "        ## Open the image and resize to appropriatee size\n",
    "        image = Image.open(fn)\n",
    "        image = image.resize((299, 299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        image = image / 127.5\n",
    "        image = image - 1.0\n",
    "        \n",
    "        ## predict and store efeature \n",
    "        feature = Xmodel.predict(image)\n",
    "        features[img] = feature\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956a0ec97b9b4113851456d463b2a17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-70feab1ba3a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 2048 feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeaturse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "## 2048 feature vector\n",
    "features = extract_features(dataset_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(features, open('features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_pickle('features.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset to train model\n",
    "\n",
    "In `data/Flickr_8k_test/Flickr_8k.trainImages.txt`, there are 6000 image names that will be useed for training.\n",
    "\n",
    "**load_photos( filename )** – This will load the text file in a string and will return the list of image names.\n",
    "\n",
    "**load_clean_descriptions( filename, photos )** – This function will create a dictionary that contains captions for each photo from the list of photos. We also append the `<start>` and `<end>` identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.\n",
    "    \n",
    "**load_features(photos)** - This funciton will give us the dictionary for image names and their feature vector which we have previously extraccted from the Xception model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data from a file.\n",
    "'''\n",
    "def load_photos(fn):\n",
    "    file = load_doc(fn)\n",
    "    photos = file.split('\\n')[:-1]\n",
    "    return photos\n",
    "\n",
    "'''\n",
    "Creates a dictionary that contains captions for each photo from the list of photos. \n",
    "Appends a `start` and `end` tag for each caption so that LSTM will be able to denote.\n",
    "'''\n",
    "def load_clean_descriptions(fn, photos):\n",
    "    ## Load cleaed_descriptoins \n",
    "    file = load_doc(fn)\n",
    "    descriptions = {}\n",
    "    for line in file.split('\\n'):\n",
    "        \n",
    "        ## Extract the image and image caption that we found from CNN\n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "            \n",
    "        image, image_caption = words[0], words[1:]\n",
    "        \n",
    "        ## Include the start and end tags as needeed\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + ' '.join(image_caption) + ' <ends>'\n",
    "            descriptions[image].append(desc)\n",
    "            \n",
    "    return descriptions\n",
    "\n",
    "'''\n",
    "Give us the dictionary for image names and their feature vector\n",
    "which we have previously extraccted from the Xception model.\n",
    "'''\n",
    "def load_features(photos):\n",
    "    ## Load all features \n",
    "    all_features = load(open('features.p', 'rb'))\n",
    "    \n",
    "    ## select only the necessary features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fn = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "\n",
    "train_imgs = load_photos(training_fn)\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Vocabulary\n",
    "\n",
    "We need to represent each unique vocab word with a unique index value. This uses the Keras library's tokenizer function and save it as `tokenizer.p` pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts dictionary to clean list of descriptions.\n",
    "'''\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    ## Collect all the values in the descriptions and append it to all_desc\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "        \n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a tokenizer class, which vectorizes text body.\n",
    "Each integer maps to token in dictionary.\n",
    "'''\n",
    "def create_tokenizer(descriptions):\n",
    "    ## collect the list of descriptions\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    \n",
    "    ## initialie Keras Tokenizer and then fit on the fonud descriptions\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire vocab size:  7577\n"
     ]
    }
   ],
   "source": [
    "## Each word gets an index, which is stored in the `tokenizer.p` pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Entire vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also find the max_length of the descriptions so that model structure knows\n",
    "\n",
    "'''\n",
    "Calculates maximum length of the descriptions\n",
    "'''\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max([len(d.split()) for d in desc_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max_length(descriptions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "Train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.\n",
    "\n",
    "Generator will yield input and output sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create input-output sequence pairs from the image description\n",
    "'''\n",
    "Data generator, used by model.fit_generator()\n",
    "'''\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    while True:\n",
    "        for key, description_list in descriptions.items():\n",
    "            # Retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            \n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, \n",
    "                                                                         max_length, \n",
    "                                                                         description_list, \n",
    "                                                                         feature)\n",
    "            ## yield the sequence result\n",
    "            yield [[input_image, input_sequence], output_word]\n",
    "            \n",
    "'''\n",
    "Tokenizes the descriptions into multiple X, y pairs \n",
    "so that features, input, and output can be separated\n",
    "'''\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    \n",
    "    ## Go through each description for the image\n",
    "    for desc in desc_list:\n",
    "        ## Encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        \n",
    "        ## Split sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            ## Split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            \n",
    "            ## Pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            \n",
    "            ## Encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            \n",
    "            ## Store the input and output \n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            \n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47, 2048), (47, 32), (47, 7577))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## can vereify shape of input and output\n",
    "[a,b], c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n",
    "\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining CNN-RNN model\n",
    "\n",
    "Using Keras Model from Functional API, the model is mad eup of 3 major components:\n",
    "\n",
    "1. **Feature Extractor** – The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.\n",
    "\n",
    "2. **Sequence Processor** – An embedding layer will handle the textual input, followed by the LSTM layer.\n",
    "\n",
    "3. **Decoder** – By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size.\n",
    "\n",
    "![Input layers](images/CNN-RNN-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define captioning model\n",
    "\n",
    "def define_model(vocab_size, max_length, summary_fn='model.png'):\n",
    "    ## Features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(.5)(inputs1) ## prevents overfitting\n",
    "    fe2 = Dense(256, activation='relu')(fe1) ## deeply connected\n",
    "    \n",
    "    ## LSTM Sequence Model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    ## Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    ## all together, compiled \n",
    "    model = Model(inputs=[inputs1, inputs2],\n",
    "                 outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam')\n",
    "    \n",
    "    ## Summarizer model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=summary_fn, show_shapes=True)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN-RNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0004992485046386719"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"hi\")\n",
    "print()\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7577\n",
      "Description Length:  32\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 32, 256)      1939712     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 2048)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 256)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 256)          524544      dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          525312      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 256)          0           dense_13[0][0]                   \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 256)          65792       add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 7577)         1947289     dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,002,649\n",
      "Trainable params: 5,002,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 845s 141ms/step - loss: 4.4790\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 831s 139ms/step - loss: 3.6449\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 830s 138ms/step - loss: 3.3659\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 839s 140ms/step - loss: 3.2001\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 832s 139ms/step - loss: 3.0863\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 843s 141ms/step - loss: 3.0027\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 822s 137ms/step - loss: 2.9426\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 838s 140ms/step - loss: 2.8898\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 832s 139ms/step - loss: 2.8501\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 801s 133ms/step - loss: 2.8143\n",
      "Finished in :  8320.344820022583\n"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(train_descriptions)\n",
    "\n",
    "# making a directory models to save our models\n",
    "# os.mkdir(\"models\")\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")\n",
    "\n",
    "finished = time.time() - start\n",
    "print('Finished in : ', finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a separate `testing_caption_generator.py` thatt will load the model and generate predictions.\n",
    "Predictions contain max length of index values so we use the same `tokenizer.py` pickle file to get words from their index values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://data-flair.training/blogs/python-based-project-image-caption-generator-cnn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
